from pathlib import Path
from typing import List, Union, Tuple, Dict
import pickle
import re
from datetime import datetime as dt
from tqdm import tqdm

import pandas as pd
import numpy as np
import tweepy
import emoji
import neologdn
import MeCab

import myutil


prj_path = myutil.PROJECT_PATH


def log(msg: Union[str, Path], exception=False) -> str:
    suffix = '[{}] '.format(Path(__name__))
    if isinstance(msg, Path):
        msg = str(msg)

    if exception:
        Exception(suffix + msg)
    else:
        print(suffix + msg)


def verify_tweepy(wait_on_rate_limit=True, wait_on_rate_limit_notify=True) -> tweepy.API:
    """Tweepyã«ã‚ˆã‚‹ãƒ¦ãƒ¼ã‚¶èªè¨¼

    Args:
        wait_on_rate_limit (bool, optional): ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®è£œå……ã‚’ã‚ªãƒ¼ãƒˆå¾…æ©Ÿ. Defaults to True.
        wait_on_rate_limit_notify (bool, optional): ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®è£œå……å¾…æ©Ÿæ™‚ã®é€šçŸ¥. Defaults to True.

    Returns:
        tweepy.API: Twitter API Wrapper
    """
    # APIã‚­ãƒ¼èª­ã¿è¾¼ã¿
    api_key, api_secret, access_token, access_secret = myutil.load_twitter_api_keys()

    # ãƒ¦ãƒ¼ã‚¶èªè¨¼(OAuth)
    auth = tweepy.OAuthHandler(api_key, api_secret)
    auth.set_access_token(access_token, access_secret)
    api = tweepy.API(auth, wait_on_rate_limit=wait_on_rate_limit,
                     wait_on_rate_limit_notify=wait_on_rate_limit_notify)

    return api


class Holomem:
    """ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ¡ãƒ³ãƒãƒ¼
    """

    def __init__(self, userId: str, n_tweet: int, load=True, verbose=False, preview=False) -> None:
        """ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–

        Args:
            userId (str): Twitter IDï¼ˆãƒ¦ãƒ¼ã‚¶IDï¼‰
            n_tweet (int): å–å¾—ãƒ„ã‚¤ãƒ¼ãƒˆæ•°
            load (bool, optional): æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ï¼ˆäºˆæœŸã›ã¬APIå‘¼ã³å‡ºã—å›é¿ã®ãŸã‚ï¼‰. Defaults to True.
            verbose (bool, optional): ä¿å­˜ãƒ‘ã‚¹ãªã©ã®æƒ…å ±å‡ºåŠ›. Defaults to False.
            preview (bool, optional): å–å¾—ãƒ»èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­éƒ¨è¡¨ç¤ºï¼ˆ5ãƒ¬ã‚³ãƒ¼ãƒ‰ï¼‰. Defaults to False.
        """
        # æŒ‡å®šã—ãŸãƒ¦ãƒ¼ã‚¶IDã®ãƒ¡ãƒ³ãƒãƒ¼æƒ…å ±å–å¾—
        hololive_members = pd.read_csv(Path(prj_path, 'data/hololive_members.csv'))
        member_info = hololive_members[hololive_members['twitter'] == userId]

        self.userId = member_info.twitter.values[0]  # ãƒ¦ãƒ¼ã‚¶ID
        self.name = member_info.name.values[0]      # ãƒ•ãƒ«ãƒãƒ¼ãƒ 
        self.generation = set(member_info.generation.values[0].split('/'))  # ä¸–ä»£ï¼ˆnæœŸ/ã‚²ãƒ¼ãƒãƒ¼ã‚ºï¼‰

        self.n_tweet = n_tweet
        self.load = load
        self.verbose = verbose
        self.preview = preview

        # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ãƒ‘ã‚¹
        self.tweets_path = Path(prj_path, 'data/tweets_{0}_{1:d}.pkl'.format(self.userId, self.n_tweet))
        # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ä¿å­˜ãƒ‘ã‚¹
        self.tweets_df_path = Path(prj_path, 'data/tweets_{0}_{1:d}.tsv'.format(self.userId, self.n_tweet))
        self.tweets = self.get_tweets(save=True)    # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—
        self.df = self.create_tweets_df(save=True)  # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ

    def get_tweets(self, save=True) -> List[tweepy.models.Status]:
        """ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—

        Args:
            save (bool, optional): ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®pklå½¢å¼ã§ã®ä¿å­˜. Defaults to True.

        Returns:
            List[tweepy.models.Status]: ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ„ã‚¤ãƒ¼ãƒˆã«é–¢ã™ã‚‹ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ä¿æŒï¼‰
        """
        api = verify_tweepy()   # APIãƒ¦ãƒ¼ã‚¶èªè¨¼

        if self.load or self.tweets_path.exists():
            # æ—¢å­˜ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
            with open(self.tweets_path, mode='rb') as f:
                tweets = pickle.load(f)
            if self.verbose:
                log('ä»¥ä¸‹ãƒ‘ã‚¹ã‚ˆã‚Šãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ')
                log('{}'.format(self.tweets_path))
        else:
            # APIã‚’åˆ©ç”¨ã—ãŸãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿å–å¾—
            tweets = [
                tweet for tweet in tweepy.Cursor(api.user_timeline,
                                                 screen_name=self.userId, include_rts=True).items(self.n_tweet)]
            if save:
                with open(self.tweets_path, mode='wb') as f:
                    pickle.dump(tweets, f)
                if self.verbose:
                    log('ä»¥ä¸‹ãƒ‘ã‚¹ã«ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ')
                    log('{}'.format(self.tweets_path))

        if self.preview:
            # å–å¾—ã§ããŸãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®å…ˆé ­5ã¤è¡¨ç¤ºï¼ˆæœ€æ–°5ãƒ„ã‚¤ãƒ¼ãƒˆï¼‰
            for tweet in tweets[:5]:
                print('-------------------------')
                print(tweet.created_at)
                print(tweet.text)

        return tweets

    def create_tweets_df(self, save=True) -> pd.core.frame.DataFrame:
        """ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ

        Args:
            save (bool, optional): ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®csvå½¢å¼ã§ã®ä¿å­˜. Defaults to True.

        Returns:
            pd.core.frame.DataFrame: ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆãƒ¦ãƒ¼ã‚¶IDï¼ŒæŠ•ç¨¿æ—¥æ™‚ï¼Œãƒ†ã‚­ã‚¹ãƒˆï¼‰
        """
        if self.tweets_df_path.exists():
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿
            tweets_df = pd.read_csv(self.tweets_df_path, sep='\t', index_col=0, parse_dates=[2])
        else:
            # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆ
            # ï¼ˆãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ„ã‚¤ãƒ¼ãƒˆæ•°ã¯ä¸Šé™ã«é”ã™ã‚‹ã“ã¨ã§n_tweetã‚ˆã‚Šå°‘ãªããªã‚‹ã“ã¨ãŒã‚ã‚‹ãŸã‚ï¼Œ
            # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’å–å¾—ã™ã‚‹ï¼‰
            tweets_data = [[] for _ in range((len(self.tweets)))]
            for idx, tweet in enumerate(self.tweets):
                data = [tweet.user.screen_name, tweet.created_at, tweet.text]
                tweets_data[idx] = data

            tweets_df = pd.DataFrame(tweets_data, columns=['userId', 'timestamp', 'text'])

            if save:
                tweets_df.to_csv(self.tweets_df_path, sep='\t', index=True)

        if self.preview:
            print(tweets_df.head(5))

        return tweets_df

    def format_tweets_df(self, save=True) -> None:
        """ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æ•´å½¢ï¼ˆtextåˆ—ã®å‰å‡¦ç†ï¼‰

        Args:
            save (bool, optional): ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®csvå½¢å¼ã§ã®ä¸Šæ›¸ãä¿å­˜. Defaults to True.
        """
        tweets_df = self.df.copy()

        # RTã‚’å‰Šé™¤
        tweets_df_without_rt = tweets_df[tweets_df['text'].str[:2] != 'RT'].copy()
        # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆï¼ˆtextåˆ—ï¼‰æ•´å½¢ï¼ˆè©³ç´°ã¯format_tweet_text()å‚ç…§ï¼‰
        tweets_df_without_rt['text'] = tweets_df_without_rt['text'].map(format_tweet_text).copy()
        # å†…å®¹ãŒãªã„ãƒ„ã‚¤ãƒ¼ãƒˆå‰Šé™¤
        tweets_df_without_empty = tweets_df_without_rt[~tweets_df_without_rt['text'].isin([' ', ''])].copy()
        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ æ›´æ–°
        self.df = tweets_df_without_empty

        if save:
            tweets_df_without_empty.to_csv(self.tweets_df_path, sep='\t', index=True)

        if self.preview:
            print(tweets_df.head(5))


def format_tweet_text(tweet_text: str, return_original=False) -> Union[str, Tuple[str, str]]:
    """ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢

    Args:
        tweet_text (str): ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        return_original (bool, optional): æ•´å½¢å‰ã®ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚‚è¿”ã™. Defaults to False.

    Returns:
        Union[str, Tuple[str, str]]: æ•´å½¢å¾Œãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆï¼ˆreturn_original=Trueã§æ•´å½¢å‰ã®ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚‚è¿”ã™ï¼‰
    """
    # URLé™¤å»
    formatted_tweet_text = re.sub(r'https?://[\w/:%#\$&\?\(\)~\.=\+\-]+', '', tweet_text)
    # çµµæ–‡å­—é™¤å»
    formatted_tweet_text = ''.join(['' if c in emoji.UNICODE_EMOJI['en'] else c for c in formatted_tweet_text])
    # æ­£è¦åŒ–
    formatted_tweet_text = neologdn.normalize(formatted_tweet_text)
    # ãƒªãƒ—ãƒ©ã‚¤å…ˆï¼ˆï¼ ãƒ¦ãƒ¼ã‚¶åï¼‰ã®é™¤å»
    formatted_tweet_text = re.sub(r'@[ ]?[a-zA-z0-9]+', '', formatted_tweet_text)
    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°é™¤å»
    formatted_tweet_text = re.sub(r'[#][A-Za-zä¸€-é¿†0-9ã-ãƒ¶ï½¦-ï¾Ÿãƒ¼]+', '', formatted_tweet_text)
    # æ”¹è¡Œé™¤å»
    formatted_tweet_text = re.sub(r'[\n]', ' ', formatted_tweet_text)
    # åŠè§’è¨˜å·é™¤å»
    formatted_tweet_text = re.sub(r'[!\"\#$%&\'()*+,-./:;<=>?@[\]^_`{|}~\\]', ' ', formatted_tweet_text)
    # å…¨è§’è¨˜å·é™¤å»
    formatted_tweet_text = re.sub(r'[ãƒ»ã€œï¼‹ï½œâ€™ï¼›ï¼šï¼ï¼œã€ã€Œï¼ˆï¼‰ï¼…ï¼„ï¼ƒï¼ ï¼†ï¼¾ï¼Šï¼ï¼Ÿã€ã€‘ã€ã€ï¼¼ï¼â€¦â—‹â–³â–¡â—â–²â– â–¼â–¶â—€â–½â˜…â˜†â€»â€¥]',
                                  '', formatted_tweet_text)
    # æ•°å­—é™¤å»
    formatted_tweet_text = re.sub(r'[0-9]', '', formatted_tweet_text)
    # é™¤è‰
    formatted_tweet_text = re.sub(r'[w]{2,}', ' ', formatted_tweet_text)
    # ç‰¹æ®Šè¨˜å·é™¤å»
    formatted_tweet_text = re.sub(r'[à¸Ë¶á·‡ á·†Ëµá¯…Ê…Êƒâ–“à¥â–·Â§Ù©à¸‡á â€¢à¹‘à¸§â‰¡ï¿£âƒ•áŸğ“¸â—à½«á†ºÌã……ÎµÏˆÎ¶âˆ â—†à¼¥Â³â™¡Ì©Ë˜â—¥ï¿¢á”¦âœ¿ï¹€â—¦âŒ“â”ƒâ‚â‚â¾â¾  à¥ƒà¹› ÍŸÍŸÍá”¨â¸œã…‚â—¤ê’³ã€‚â—Ÿï¾Ÿï½€Ê–â””á´—Â´â‹†Ëšà¬˜â¸â•¹â”‚ê™¬Ì®ğ“‚‚â–¹â–¸Â°â†ÍœÌ€à©­ã„˜Î¶ÙˆÌ‘Ì‘ß¹ã€ƒÂ³Â¦à¼¥Ë™ê‡¤êœ„êœ† âƒ”Ğ·à©ˆà¥‘â‚ŠÊ˜ Ï‰á—œâŠ‚â€§á›âŒ’â™†â›áµ•âœ©â™ªâ—¡â–€Ì¿ãƒ¾ğ–¥¦áµ’Ì´Ì¶Ì·âœ§Ë†ËŠË‹]',
                                  ' ', formatted_tweet_text)
    # é€£ç¶šåŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã‚’ã¾ã¨ã‚ã‚‹
    formatted_tweet_text = re.sub(r'[ ]+', ' ', formatted_tweet_text)

    if return_original:
        return formatted_tweet_text, tweet_text
    else:
        return formatted_tweet_text


class Hololive:
    """ãƒ›ãƒ­ãƒ©ã‚¤ãƒ–ãƒ¡ãƒ³ãƒãƒ¼ã®ã‚¯ãƒ©ã‚¹Holomemã®æŒã¤ãƒ‡ãƒ¼ã‚¿ã‚’é›†ç´„ã™ã‚‹
    """

    def __init__(self, userIds: np.ndarray, n_tweet: int, fname: str) -> None:
        """ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–

        Args:
            userIds (np.ndarray): Twitter IDï¼ˆãƒ¦ãƒ¼ã‚¶IDï¼‰
            n_tweet (int): å–å¾—ãƒ„ã‚¤ãƒ¼ãƒˆæ•°
            fname (str): ãƒ•ã‚¡ã‚¤ãƒ«åæ¥é ­è¾
        """
        self.userIds = userIds
        self.n_tweet = n_tweet
        self.fname = fname

        # userIdsã§æŒ‡å®šã—ãŸãƒ›ãƒ­ãƒ¡ãƒ³ã®Holomem.dfã‚’é›†ç´„ã—ãŸè¾æ›¸å–å¾—ï¼ˆã‚­ãƒ¼ã¯ãƒ¦ãƒ¼ã‚¶IDï¼‰
        self.tweets_dfs = self.get_tweets_dfs()
        # tweets_dfsã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é€£çµï¼†æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—
        self.all_user_tweets_df = self.get_all_user_tweets_df()

    def get_tweets_dfs(self) -> Dict[str, Holomem]:
        """userIdsã§æŒ‡å®šã—ãŸãƒ›ãƒ­ãƒ¡ãƒ³ã®Holomem.dfã‚’é›†ç´„ã—ãŸè¾æ›¸ã‚’å–å¾—ã™ã‚‹

        Returns:
            Dict[str, Holomem]: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ é›†ç´„è¾æ›¸
        """
        tweets_dfs = dict()
        for userId in self.userIds:
            holomem = Holomem(userId=userId, n_tweet=self.n_tweet, load=True, verbose=False, preview=False)
            # æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆ -> æ¬ æè¡Œå‰Šé™¤ -> ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
            tweets_dfs[userId] = holomem.df.iloc[::-1].dropna(how='any', axis=0).reset_index(drop=True)

        return tweets_dfs

    def get_all_user_tweets_df(self) -> pd.core.frame.DataFrame:
        """tweets_dfsã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é€£çµï¼†æ™‚ç³»åˆ—é †ã«ã‚½ãƒ¼ãƒˆã—ãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã™ã‚‹

        Returns:
            pd.core.frame.DataFrame: é€£çµãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        """
        dfs = self.tweets_dfs.values()
        # æ™‚ç³»åˆ—ã‚½ãƒ¼ãƒˆ -> ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚»ãƒƒãƒˆ
        all_user_tweets_df = pd.concat(dfs, axis=0).sort_values(
            ['timestamp', 'userId'], ascending=[True, True]).reset_index(drop=True)
        return all_user_tweets_df

    def __wakatigaki(self) -> None:
        """[private] ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®åˆ†ã‹ã¡æ›¸ãã‚’è¡Œã†
        """
        # MeCabè¾æ›¸ -> NEologd
        neologd = MeCab.Tagger('-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')
        one_word_tweet_idx = []  # ãƒ†ã‚­ã‚¹ãƒˆãŒ2å˜èªæœªæº€ã®ãƒ„ã‚¤ãƒ¼ãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        all_user_tweets_df = self.all_user_tweets_df.copy()

        # åˆ†ã‹ã¡æ›¸ãçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒæ—¢ã«å­˜åœ¨ -> å‰Šé™¤ã—ã¦ä½œæˆã—ç›´ã™
        if self.wakatigaki_path.exists():
            self.wakatigaki_path.unlink()

        with open(self.wakatigaki_path, mode='a', encoding='utf-8') as f:
            for tweet in all_user_tweets_df.itertuples():
                idx, text = tweet.Index, tweet.text
                neologd_text = neologd.parse(text)  # Chasenå½¢å¼ã§åˆ†ã‹ã¡æ›¸ã
                # å˜èªã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§è¿½åŠ ï¼ˆå“è©='è¨˜å·-ä¸€èˆ¬'ãƒ»EOFé™¤ãï¼‰
                wakati_words = [word.split('\t')[0] for word in neologd_text.split('\n')
                                if len(word.split('\t')) == 6 and word.split('\t')[3] != 'è¨˜å·-ä¸€èˆ¬']

                # ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ§‹æˆã™ã‚‹å˜èªæ•°ãŒ2æœªæº€ -> one_word_tweet_idxã«è¿½åŠ 
                if len(wakati_words) < 2:
                    one_word_tweet_idx.append(idx)
                    continue

                # å˜èªã”ã¨ã«ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šï¼Œãƒ„ã‚¤ãƒ¼ãƒˆã”ã¨ã«æ”¹è¡ŒåŒºåˆ‡ã‚Šã§ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿
                wakati_text = ' '.join(wakati_words)
                f.write(wakati_text)
                f.write('\n')

        # one_word_tweet_idxã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾å¿œã™ã‚‹è¡Œï¼ˆãƒ„ã‚¤ãƒ¼ãƒˆï¼‰ã‚’all_user_tweets_dfã‹ã‚‰å‰Šé™¤
        all_user_tweets_df.drop(index=one_word_tweet_idx, inplace=True)
        self.all_user_tweets_df = all_user_tweets_df

    def split_data(self, date: dt, save=True, verbose=False) -> Tuple[
            pd.core.frame.DataFrame, pd.core.frame.DataFrame]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«åˆ†å‰²ã™ã‚‹

        Args:
            date (dt): åˆ†å‰²æ—¥æ™‚
            save (bool, optional): åˆ†å‰²ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®tsvå½¢å¼ã§ã®ä¿å­˜. Defaults to True.
            verbose (bool, optional): ä¿å­˜ãƒ‘ã‚¹ãªã©ã®æƒ…å ±å‡ºåŠ›. Defaults to False.

        Returns:
            Tuple[ pd.core.frame.DataFrame, pd.core.frame.DataFrame]: è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ï¼‰
        """
        self.change_date(date=date, verbose=False)  # åˆ†å‰²æ—¥è¨­å®šï¼ˆä¿å­˜ãƒ‘ã‚¹æŒ‡å®šï¼‰
        self.__wakatigaki()                         # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã®åˆ†ã‹ã¡æ›¸ã
        all_user_tweets_df = self.all_user_tweets_df.copy()

        # all_user_tweets_dfã®åˆ†å‰²ï¼ˆåˆ†å‰²æ—¥æ™‚ä¸Šã®ãƒ„ã‚¤ãƒ¼ãƒˆã¯ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«å…¥ã‚‹ï¼‰
        train_df = all_user_tweets_df[all_user_tweets_df['timestamp'] < date]
        test_df = all_user_tweets_df[all_user_tweets_df['timestamp'] >= date]

        if save:
            train_df.to_csv(self.train_df_path, sep='\t', index=True)
            test_df.to_csv(self.test_df_path, sep='\t', index=True)

        self.train_df = train_df
        self.test_df = test_df

        # è¨“ç·´ã‚»ãƒƒãƒˆã«å¯¾å¿œã™ã‚‹ã‚ˆã†ã«åˆ†ã‹ã¡æ›¸ãçµæœãƒ•ã‚¡ã‚¤ãƒ«ã®æœ«å°¾ï¼ˆãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆéƒ¨ï¼‰ã‚’ã‚«ãƒƒãƒˆ
        with open(self.wakatigaki_path, mode='r', encoding='utf-8') as f:
            wakati_text = f.readlines()
        wakati_text_train = ''.join(wakati_text[:train_df.shape[0]])
        with open(self.wakatigaki_path, mode='w', encoding='utf-8') as f:
            f.write(wakati_text_train)

        if verbose:
            log('ä»¥ä¸‹ãƒ‘ã‚¹ã«åˆ†ã‹ã¡æ›¸ãçµæœã¨ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ')
            log('åˆ†ã‹ã¡æ›¸ãçµæœï¼ˆè¨“ç·´ã‚»ãƒƒãƒˆï¼‰: {}'.format(self.wakatigaki_path))
            log('è¨“ç·´ã‚»ãƒƒãƒˆ: {}'.format(self.train_df_path))
            log('ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ: {}'.format(self.test_df_path))

        return train_df, test_df

    def __get_session_data(self, df: pd.core.frame.DataFrame) -> List[Tuple[int, str, str, dt]]:
        """ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®ãƒªã‚¹ãƒˆï¼ˆsession_dataï¼‰ã‚’å–å¾—ã™ã‚‹

        Args:
            df (pd.core.frame.DataFrame): ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 

        Returns:
            List[Tuple[int, str, str, dt]]: ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®ãƒªã‚¹ãƒˆ
        """
        neologd = MeCab.Tagger('-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')
        session_data = []

        for tweet in tqdm(df.itertuples(), total=df.shape[0]):
            # å„ãƒ„ã‚¤ãƒ¼ãƒˆã‚’å˜èªã”ã¨ã«è¡Œã‚’åˆ†ã‘ã¦ãƒ­ã‚°ã‚’ä½œæˆ -> session_dataã«è¿½åŠ 
            neologd_text = neologd.parse(tweet.text)
            words = [word.split('\t')[0] for word in neologd_text.split('\n')
                     if len(word.split('\t')) == 6 and word.split('\t')[3] != 'è¨˜å·-ä¸€èˆ¬']

            tId, uId, timestamp = tweet.Index, tweet.userId, tweet.timestamp
            tweet_logs = [[tId, uId, word, timestamp] for word in words]
            session_data.extend(tweet_logs)

        return session_data

    def create_session_df(self, train_test=[True, True], save=True, verbose=False
                          ) -> Union[pd.core.frame.DataFrame,
                                     Tuple[pd.core.frame.DataFrame, pd.core.frame.DataFrame]]:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹

        Args:
            train_test (list, optional): Trueã«è¨­å®šã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãã‚Œãã‚Œã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹. \
                Defaults to [True, True].
            save (bool, optional): ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®csvå½¢å¼ã§ã®ä¿å­˜. Defaults to True.
            verbose (bool, optional): ä¿å­˜ãƒ‘ã‚¹ãªã©ã®æƒ…å ±å‡ºåŠ›. Defaults to False.

        Returns:
            Union[pd.core.frame.DataFrame, Tuple[pd.core.frame.DataFrame, pd.core.frame.DataFrame]]: \
                train_testã®æŒ‡å®šã«å¯¾å¿œã™ã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿
        """
        train_df, test_df = self.train_df.copy(), self.test_df.copy()
        session_df_columns = ['tweetId', 'userId', 'word', 'timestamp']

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿å½¢å¼ã®ãƒªã‚¹ãƒˆã‚’ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å½¢å¼ã«å¤‰æ›
        if train_test[0]:
            train_session_data = self.__get_session_data(df=train_df)
            train_session_df = pd.DataFrame(train_session_data, columns=session_df_columns)
            self.train_session_df = train_session_df
        if train_test[1]:
            test_session_data = self.__get_session_data(df=test_df)
            test_session_df = pd.DataFrame(test_session_data, columns=session_df_columns)
            self.test_session_df = test_session_df

        if save:
            if train_test[0]:
                train_session_df.to_csv(self.train_session_df_path, index=True)
                if verbose:
                    log('ä»¥ä¸‹ãƒ‘ã‚¹ã«è¨“ç·´ã‚»ãƒƒãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ')
                    log(self.train_session_df_path)

            if train_test[1]:
                test_session_df.to_csv(self.test_session_df_path, index=True)
                if verbose:
                    log('ä»¥ä¸‹ãƒ‘ã‚¹ã«ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ')
                    log(self.test_session_df_path)

        if train_test[0] and train_test[1]:
            return train_session_df, test_session_df
        elif train_test[0]:
            return train_session_df
        elif train_test[1]:
            return test_session_df
        else:
            log('train_testã¯å¿…ãšã©ã¡ã‚‰ã‹ã¯Trueã«ã—ã¦ãã ã•ã„', exception=True)

    def __calc_session_len(self, df: pd.core.frame.DataFrame) -> Tuple[float, int, int]:
        """[private] ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³é•·ã®å¹³å‡ãƒ»æœ€å¤§ãƒ»æœ€å°ã‚’è¨ˆç®—ã™ã‚‹

        Args:
            df (pd.core.frame.DataFrame): [description]

        Returns:
            Tuple[float, int, int]: [description]
        """
        # tweetIdã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ– -> å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚µã‚¤ã‚ºã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ 
        sessions_len = np.array([tId_df.shape[0] for _, tId_df in df.groupby('tweetId')])
        return np.mean(sessions_len), np.max(sessions_len), np.min(sessions_len)

    def show_session_len(self) -> None:
        """ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³é•·ã®å¹³å‡ãƒ»æœ€å¤§ãƒ»æœ€å°ã®è¨ˆç®—çµæœã‚’è¡¨ç¤ºã™ã‚‹
        """
        if self.train_session_df is not None:
            train_session_df = self.train_session_df.copy()
            avg_len, max_len, min_len = self.__calc_session_len(train_session_df)
            log('è¨“ç·´ã‚»ãƒƒãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³é•·æƒ…å ±')
            print('Average:{0} / Max:{1} / Min:{2}'.format(avg_len, max_len, min_len))
        if self.test_session_df is not None:
            test_session_df = self.test_session_df.copy()
            avg_len, max_len, min_len = self.__calc_session_len(test_session_df)
            log('ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®ã‚»ãƒƒã‚·ãƒ§ãƒ³é•·æƒ…å ±')
            print('Average:{0} / Max:{1} / Min:{2}'.format(avg_len, max_len, min_len))

    def change_date(self, date: dt, verbose=False) -> None:
        """åˆ†å‰²æ—¥æ™‚ã¨ä¿å­˜ãƒ‘ã‚¹ã‚’å¤‰æ›´ã™ã‚‹

        Args:
            date (dt): åˆ†å‰²æ—¥æ™‚
            verbose (bool, optional): ä¿å­˜ãƒ‘ã‚¹ãªã©ã®æƒ…å ±å‡ºåŠ›. Defaults to False.
        """
        # å¤‰æ›´å‰ã®åˆ†å‰²æ—¥æ™‚ã‚’ä¿æŒ
        if 'date' in vars(self).keys():
            prv_date = self.date
        else:
            prv_date = None
        self.date = date
        date_str = date.strftime('%Y%m%d')  # datetimeã®å‡ºåŠ›å½¢å¼æŒ‡å®š

        # ãƒ•ã‚¡ã‚¤ãƒ«å: ${fname}_${n_tweet}_${date}_{train/test}_{session}
        self.wakatigaki_path = Path(prj_path, 'data/wakatigaki/{0}_{1}_{2}.txt'.format(
            self.fname, self.n_tweet, date_str))    # åˆ†ã‹ã¡æ›¸ãçµæœ (txt)
        self.train_df_path = Path(prj_path, 'data/dataset/{0}_{1}_{2}_train.tsv'.format(
            self.fname, self.n_tweet, date_str))    # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿: è¨“ç·´ã‚»ãƒƒãƒˆ (tsv)
        self.test_df_path = Path(prj_path, 'data/dataset/{0}_{1}_{2}_test.tsv'.format(
            self.fname, self.n_tweet, date_str))    # ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿: ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ (tsv)
        self.train_session_df_path = Path(prj_path, 'data/session/{}'.format(
            self.train_df_path.stem + '_session.csv'))  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿: è¨“ç·´ã‚»ãƒƒãƒˆ (csv)
        self.test_session_df_path = Path(prj_path, 'data/session/{}'.format(
            self.test_df_path.stem + '_session.csv'))   # ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿: ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ (csv)

        if verbose:
            if prv_date is None:
                log('è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®åˆ†å‰²æ—¥ã‚’"{}"ã«è¨­å®šã—ã¾ã—ãŸ'.format(date_str))
            else:
                log('è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®åˆ†å‰²æ—¥ã‚’"{0}"ã‹ã‚‰"{1}"ã«å¤‰æ›´ã—ã¾ã—ãŸ'.format(
                    prv_date.strftime('%Y%m%d'), date_str))
